{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd03755395980d0e6f63e5e1ff4a59e0f760d2fd6a190af202581564a73c5653cef",
   "display_name": "Python 3.7.4 64-bit ('kibum': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 1024,\n",
    "    'path' : './',\n",
    "    'neg_item' : 1,\n",
    "    'n_embedding' : 64,\n",
    "    'topk' : 10,\n",
    "    'core' : 8,\n",
    "    'regularization' : 0.005,\n",
    "    'lr' : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:96: RuntimeWarning: divide by zero encountered in power\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(config['core'])\n",
    "train_data = EpinionData(config)\n",
    "train_data.load_data()\n",
    "model = NGCF(train_data.n_user, train_data.n_item, n_embedding=config['n_embedding']).cuda()\n",
    "test_data = EpinionTest(config)\n",
    "test_data.load_test()\n",
    "test_dataloader = DataLoader(test_data, batch_size = config['batch_size']*2, shuffle=False) \n",
    "optim = torch.optim.Adam(model.parameters(), lr = config['lr'])\n",
    "sparse_eye = sp.eye(train_data.n_user+train_data.n_item, dtype = np.float)\n",
    "sparse_eye = sparse_mx_to_torch_sparse_tensor(sparse_eye).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch : 0, Loss : 14.602896, Time : 1.084796\n",
      "Epoch : 0, Hit@10 : 0.000512,NDCG@10 : 0.000604 Time : 3.520539\n",
      "Epoch : 10, Loss : 3.149358, Time : 1.002570\n",
      "Epoch : 10, Hit@10 : 0.002016,NDCG@10 : 0.002352 Time : 3.151343\n",
      "Epoch : 20, Loss : 2.170063, Time : 1.011023\n",
      "Epoch : 20, Hit@10 : 0.001505,NDCG@10 : 0.001757 Time : 3.141455\n",
      "Epoch : 30, Loss : 1.899260, Time : 1.084933\n",
      "Epoch : 30, Hit@10 : 0.001414,NDCG@10 : 0.001641 Time : 3.158865\n",
      "Epoch : 40, Loss : 1.818218, Time : 0.839374\n",
      "Epoch : 40, Hit@10 : 0.001505,NDCG@10 : 0.001781 Time : 4.342403\n",
      "Epoch : 50, Loss : 1.778883, Time : 0.841014\n",
      "Epoch : 50, Hit@10 : 0.001475,NDCG@10 : 0.001739 Time : 4.491719\n",
      "Epoch : 60, Loss : 1.751278, Time : 0.959894\n",
      "Epoch : 60, Hit@10 : 0.001354,NDCG@10 : 0.001562 Time : 4.440440\n",
      "Epoch : 70, Loss : 1.722532, Time : 0.843796\n",
      "Epoch : 70, Hit@10 : 0.001445,NDCG@10 : 0.001689 Time : 4.417083\n",
      "Epoch : 80, Loss : 1.717094, Time : 0.837045\n",
      "Epoch : 80, Hit@10 : 0.001414,NDCG@10 : 0.001598 Time : 4.547980\n",
      "Epoch : 90, Loss : 1.702961, Time : 0.842670\n",
      "Epoch : 90, Hit@10 : 0.001745,NDCG@10 : 0.002047 Time : 4.455504\n",
      "Epoch : 100, Loss : 1.691341, Time : 0.998169\n",
      "Epoch : 100, Hit@10 : 0.001926,NDCG@10 : 0.002242 Time : 3.260898\n",
      "Epoch : 110, Loss : 1.691673, Time : 0.759594\n",
      "Epoch : 110, Hit@10 : 0.002708,NDCG@10 : 0.003150 Time : 3.116492\n",
      "Epoch : 120, Loss : 1.687436, Time : 0.757445\n",
      "Epoch : 120, Hit@10 : 0.002708,NDCG@10 : 0.003087 Time : 3.113786\n",
      "Epoch : 130, Loss : 1.683297, Time : 0.998127\n",
      "Epoch : 130, Hit@10 : 0.002949,NDCG@10 : 0.003412 Time : 3.149614\n",
      "Epoch : 140, Loss : 1.681826, Time : 1.012011\n",
      "Epoch : 140, Hit@10 : 0.002979,NDCG@10 : 0.003367 Time : 3.128403\n"
     ]
    }
   ],
   "source": [
    "for e in range(150):\n",
    "    train_data.make_batch_sampling()\n",
    "    dataloader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
    "    total_loss = 0.0\n",
    "    start = timer()\n",
    "    for user, pos, neg in dataloader:\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        user = torch.LongTensor(user).cuda()\n",
    "        pos_item = torch.LongTensor(pos).cuda()\n",
    "        neg_item = torch.LongTensor(neg).cuda()\n",
    "\n",
    "        user_embedding, item_embedding = model(train_data.L_c, sparse_eye)\n",
    "        user_batch_embed = user_embedding[user]\n",
    "        p_i_batch_embed = item_embedding[pos_item]\n",
    "        n_i_batch_embed = item_embedding[neg_item]\n",
    "        bpr_loss, reg_loss = bpr(config['batch_size'], user_batch_embed, p_i_batch_embed, n_i_batch_embed, config['regularization'])\n",
    "        loss = bpr_loss + reg_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch : {:d}, Loss : {:4f}, Time : {:4f}\".format(e, total_loss, timer()-start))\n",
    "    test_timer = timer()\n",
    "    hit, ndcg = test(model, train_data, test_dataloader, pool, sparse_eye)\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch : {:d}, Hit@{:d} : {:4f},NDCG@{:d} : {:4f} Time : {:4f}\".format(e, config['topk'],hit, config['topk'], ndcg,timer()-test_timer))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import EpinionData, EpinionTest\n",
    "from torch.utils.data import DataLoader\n",
    "import torch  \n",
    "from Model import NGCF\n",
    "from utility import bpr, test\n",
    "from timeit import default_timer as timer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bpr(batch_size, user, pos_item, neg_item, regularization):\n",
    "    pos_score = torch.sum(torch.mul(user, pos_item), dim=1)\n",
    "    neg_score = torch.sum(torch.mul(user, neg_item), dim=1)\n",
    "    loss_term = F.logsigmoid(pos_score-neg_score)\n",
    "    bpr_loss = -torch.mean(loss_term)\n",
    "    regularizer = 1./2*(user**2).sum() + 1./2*(pos_item**2).sum() + 1./2*(neg_item**2).sum()\n",
    "    regularizer = regularizer / batch_size\n",
    "    regularizer_loss = regularizer * regularization\n",
    "    return bpr_loss, regularizer_loss\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def test(model, train_module, test_dataloader, pool, eye):\n",
    "    model.eval()\n",
    "    correct = []\n",
    "    test_dataset = test_dataloader.dataset\n",
    "\n",
    "    with torch.no_grad():\n",
    "        user_embedding, item_embedding = model(train_module.L_c, eye)\n",
    "    n_user_batch = len(test_dataset.test_users) // train_module.batch_size + 1\n",
    "    total_hit_rate = []\n",
    "    total_ndcg = []\n",
    "    for index, batch_test in enumerate(test_dataloader):\n",
    "        user_batch = batch_test.cuda()\n",
    "        user_embed = user_embedding[user_batch]\n",
    "        item_embed = item_embedding[torch.arange(0, train_module.n_item, dtype = torch.long).cuda()]\n",
    "        rate_batch = torch.matmul(user_embed, torch.transpose(item_embed, 0, 1))\n",
    "        # Train에 나왔던 item은 뽑히지 않게\n",
    "        make_pos_minus(user_batch, rate_batch, train_module.train_items)\n",
    "\n",
    "        topk_item = torch.topk(rate_batch, k=train_module.topk, dim=1).indices\n",
    "        topk_item = topk_item.detach().cpu().numpy()\n",
    "        \n",
    "        for i, user in enumerate(user_batch):\n",
    "            hit, ndcg = test_one_user(user.item(), topk_item[i], test_dataset)\n",
    "            total_hit_rate += hit\n",
    "            total_ndcg.append(ndcg)\n",
    "        # user_batch_rating = zip(topk_item, user_batch)\n",
    "        # result = pool.map(test_one_user, user_batch_rating)\n",
    "        length = float(test_dataset.n_test)\n",
    "    return np.sum(total_hit_rate)/length, np.sum(total_ndcg)/model.n_user\n",
    "\n",
    "\n",
    "\n",
    "def make_pos_minus(user_batch, rate_batch, pos_train_item):\n",
    "    for i, user in enumerate(user_batch):\n",
    "        user = user.item()\n",
    "        pos_item = pos_train_item[user]\n",
    "        rate_batch[i, pos_item] = -10000.0 \n",
    "\n",
    "def test_one_user(user, topk_item, test_dataset):\n",
    "    # hit rate\n",
    "    pos_test_items = test_dataset.test_items[user]\n",
    "    hit_list = []\n",
    "    for pos in pos_test_items:\n",
    "        if pos in topk_item:\n",
    "            hit_list.append(1)\n",
    "        else:\n",
    "            hit_list.append(0)\n",
    "    # NDCG\n",
    "    reverse_list = np.asfarray(sorted(hit_list, reverse=True))\n",
    "    IDCG = np.sum(reverse_list / np.log2(np.arange(2, len(reverse_list)+2)))\n",
    "    DCG = np.sum(hit_list / np.log2(np.arange(2, len(hit_list)+2)))\n",
    "    NDCG = 0.0 if DCG == 0.0 else DCG/IDCG\n",
    "        \n",
    "\n",
    "    return hit_list, NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_embedding):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.n_embedding = n_embedding\n",
    "        self.user_embedding = nn.Embedding(self.n_user, self.n_embedding)\n",
    "        self.item_embedding = nn.Embedding(self.n_item, self.n_embedding)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        self.Front_Linear_List = nn.ModuleList()\n",
    "        self.Back_Linear_List = nn.ModuleList()\n",
    "        self.dropout_list = nn.ModuleList()\n",
    "        for _ in range(3):\n",
    "            self.Front_Linear_List.append(nn.Linear(64,64))\n",
    "            self.Back_Linear_List.append(nn.Linear(64,64))\n",
    "            self.dropout_list.append(nn.Dropout(p=0.5))\n",
    "\n",
    "    def forward(self, H, sparse_eye):\n",
    "        E_l_embedding = torch.cat((self.user_embedding.weight, self.item_embedding.weight), dim = 0)\n",
    "        all_embedding = [E_l_embedding]\n",
    "        H_I = H+sparse_eye\n",
    "        for i in range(3):\n",
    "            Front = torch.sparse.mm(H_I, E_l_embedding)\n",
    "            Front_cal = nn.functional.leaky_relu(self.Front_Linear_List[i](Front))\n",
    "            Back = torch.mul(E_l_embedding, Front)\n",
    "            Back = nn.functional.leaky_relu(self.Back_Linear_List[i](Back))\n",
    "            E_l_embedding = Front_cal + Back\n",
    "            E_l_embedding = self.dropout_list[i](E_l_embedding)\n",
    "            normalize_embed = nn.functional.normalize(E_l_embedding, p=2, dim=1)\n",
    "            all_embedding += [normalize_embed]\n",
    "        \n",
    "        all_embedding = torch.cat(all_embedding, dim=1)\n",
    "        user_embedding, item_embedding = torch.split(all_embedding, [self.n_user, self.n_item], dim=0)\n",
    "        return user_embedding, item_embedding\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpinionTest(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.n_test = 0\n",
    "        self.test_items = {}\n",
    "        self.path = config['path']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.test_items.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.test_users[index]\n",
    "\n",
    "    def load_test(self):\n",
    "        with open(self.path+'/test.txt') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) == 0:\n",
    "                    break\n",
    "                l = l.strip('\\n')\n",
    "                items = [int(i) for i in l.split(' ')]\n",
    "                self.n_test += len(items[1:])\n",
    "                self.test_items[items[0]] = items[1:]\n",
    "        self.test_users = torch.LongTensor(list(self.test_items.keys()))\n",
    "\n",
    "        \n",
    "\n",
    "class EpinionData(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.path = config['path']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.topk = config['topk']\n",
    "        self.n_user = 0\n",
    "        self.n_item = 0\n",
    "        self.n_train = 0\n",
    "        self.n_test = 0\n",
    "        self.train_items = {}\n",
    "        self.test_items = {}\n",
    "        self.R = sp.dok_matrix((10000000, 10000000), dtype=np.float32)\n",
    "        self.n_neg_item = config['neg_item']\n",
    "        self.pos_item = []\n",
    "        self.neg_item = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.exist_users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.exist_users[index], self.pos_item[index], self.neg_item[index]\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        # self.item_list = pd.read_table(self.path+'/item_list.txt', sep = '\\t', header=0)\n",
    "        # self.user_list = pd.read_table(self.path+'/user_list.txt', sep = '\\t', header=0)\n",
    "        self.exist_users = []\n",
    "        \n",
    "        with open(self.path+'/train.txt') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 2:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.R[uid, items] = 1.0\n",
    "                    self.train_items[uid] = items\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_item = max(self.n_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.n_train += len(items)\n",
    "        \n",
    "        self.n_user += 1\n",
    "        self.n_item += 1\n",
    "        self.R.resize((self.n_user, self.n_item))\n",
    "        \n",
    "      \n",
    "        self.Item_By_Item = sp.dok_matrix((self.n_item, self.n_item), dtype = np.float32)\n",
    "        self.User_By_User = sp.dok_matrix((self.n_user, self.n_user), dtype = np.float32)\n",
    "        self.H = sp.dok_matrix((self.n_user+self.n_item, self.n_user+self.n_item), dtype = np.float32)\n",
    "        self.normalize_H()\n",
    "               \n",
    "    def normalize_H(self):\n",
    "        self.H = self.H.tolil()\n",
    "        self.R = self.R.tolil()\n",
    "        self.H[:self.n_user, self.n_user:] = self.R\n",
    "        self.H[self.n_user:, :self.n_user] = self.R.T\n",
    "        # item과 user matrix를 넣으면 됨.\n",
    "        # self.H[:self.n_user, :self.n_user] = User Matrix\n",
    "        # self.H[self.n_user:, self.n_user:] = Item Matrix\n",
    "        self.H = self.H.todok()\n",
    "\n",
    "        # Normalize \n",
    "        rowsum = np.array(self.H.sum(1))\n",
    "        D_inv = np.power(rowsum, -1/2).flatten()\n",
    "        D_inv[np.isinf(D_inv)] = 0.0\n",
    "        D_ = sp.diags(D_inv)\n",
    "        self.L_c = (D_.dot(self.H)).dot(D_)\n",
    "        self.L_c = sparse_mx_to_torch_sparse_tensor(self.L_c).cuda()\n",
    "\n",
    "    def make_batch_sampling(self):\n",
    "        pos_item, neg_item = [], []\n",
    "        for user in self.exist_users:\n",
    "            pos_item += [random.choice(self.train_items[user])]\n",
    "            neg_item += self.neg_sampling(user)\n",
    "        self.pos_item = np.asarray(pos_item)\n",
    "        self.neg_item = np.asarray(neg_item).squeeze(1)\n",
    "\n",
    "\n",
    "    def neg_sampling(self, user):\n",
    "        neg = []\n",
    "        while True:\n",
    "            if len(neg) >= self.n_neg_item:\n",
    "                break\n",
    "            rand = np.random.randint(0, self.n_item, 1)\n",
    "            if rand not in self.train_items[user] and rand not in neg:\n",
    "                neg.append(rand)\n",
    "        return neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "user의 수 :  27545\n"
     ]
    }
   ],
   "source": [
    "print(\"user의 수 : \",len(train_data.exist_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction = 0\n",
    "item_set = set()\n",
    "for k,v in data.train_items.items():\n",
    "    item_set.update(v)\n",
    "    interaction += len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "item의 수 :  37279\ninteraction의 수 :  99763\n"
     ]
    }
   ],
   "source": [
    "print(\"item의 수 : \",len(item_set))\n",
    "print(\"interaction의 수 : \", interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
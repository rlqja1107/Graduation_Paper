{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 1024,\n",
    "    'path' : './Yelp',\n",
    "    'neg_item' : 1,\n",
    "    'n_embedding' : 64,\n",
    "    'topk' : 100,\n",
    "    'core' : 8,\n",
    "    'regularization' : 0.005\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pool = multiprocessing.Pool(config['core'])\n",
    "train_data = EpinionData(config)\n",
    "with open('./Epinion/train_loader', 'rb') as f:\n",
    "    loader = pickle.load(f)\n",
    "train_data.n_item = loader.n_item\n",
    "train_data.n_user = loader.n_user\n",
    "train_data.n_train = loader.n_train\n",
    "train_data.exist_users = loader.exist_users\n",
    "train_data.train_items = loader.train_items\n",
    "train_data.L_c = loader.L_c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(config['core'])\n",
    "model = NGCF(train_data.n_user, train_data.n_item, n_embedding=config['n_embedding']).cuda()\n",
    "test_data = EpinionTest(config)\n",
    "test_data.load_test()\n",
    "test_dataloader = DataLoader(test_data, batch_size = config['batch_size']*2, shuffle=False) \n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "sparse_eye = sp.eye(train_data.n_user+train_data.n_item, dtype = np.float)\n",
    "sparse_eye = sparse_mx_to_torch_sparse_tensor(sparse_eye).cuda()\n",
    "sparse_eye = sparse_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('./Epinion/train_loader', 'wb') as f:\n",
    "#     pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.28 TiB for an array with shape (2189449, 160585) and data type float32",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4540471893aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'core'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpinionData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpinionTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c8c7aac7b9e8>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUser_By_User\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdok_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdok_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_H\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_H\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c8c7aac7b9e8>\u001b[0m in \u001b[0;36mnormalize_H\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_user\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# item과 user matrix를 넣으면 됨.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, x)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# Everything else takes the normal path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mIndexMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mul_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_arrayXarray_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Make x and i into the same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m_set_arrayXarray_sparse\u001b[0;34m(self, row, col, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Fall back to densifying x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_arrayXarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.28 TiB for an array with shape (2189449, 160585) and data type float32"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(config['core'])\n",
    "train_data = EpinionData(config)\n",
    "train_data.load_data()\n",
    "model = NGCF(train_data.n_user, train_data.n_item, n_embedding=config['n_embedding'])\n",
    "test_data = EpinionTest(config)\n",
    "test_dataloader = DataLoader(test_data, batch_size = config['batch_size']*2, shuffle=False) \n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "sparse_eye = sp.eye(train_data.n_user+train_data.n_item, dtype = np.float)\n",
    "sparse_eye = sparse_mx_to_torch_sparse_tensor(sparse_eye).cuda()\n",
    "sparse_eye = sparse_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for e in range(100):\n",
    "    train_data.make_batch_sampling()\n",
    "    dataloader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
    "    total_loss = 0.0\n",
    "    start = timer()\n",
    "    for user, pos, neg in dataloader:\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        user = torch.LongTensor(user).cuda()\n",
    "        pos_item = torch.LongTensor(pos).cuda()\n",
    "        neg_item = torch.LongTensor(neg).cuda()\n",
    "\n",
    "        user_embedding, item_embedding = model(train_data.L_c, sparse_eye)\n",
    "        user_batch_embed = user_embedding[user]\n",
    "        p_i_batch_embed = item_embedding[pos_item]\n",
    "        n_i_batch_embed = item_embedding[neg_item]\n",
    "        bpr_loss, reg_loss = bpr(config['batch_size'], user_batch_embed, p_i_batch_embed, n_i_batch_embed, config['regularization'])\n",
    "        loss = bpr_loss + reg_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch : {:d}, Loss : {:4f}, Time : {:4f}\".format(e, total_loss, timer()-start))\n",
    "    test_timer = timer()\n",
    "    hit = test(model, train_data, test_dataloader, pool, sparse_eye)\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch : {:d}, Hit@{:d} : {:4f}, Time : {:4f}\".format(e, config['topk'],hit, timer()-test_timer))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import EpinionData, EpinionTest\n",
    "from torch.utils.data import DataLoader\n",
    "import torch  \n",
    "from Model import NGCF\n",
    "from utility import bpr, test\n",
    "from timeit import default_timer as timer\n",
    "import multiprocessing\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bpr(batch_size, user, pos_item, neg_item, regularization):\n",
    "    pos_score = torch.sum(torch.mul(user, pos_item), dim=1)\n",
    "    neg_score = torch.sum(torch.mul(user, neg_item), dim=1)\n",
    "    loss_term = F.logsigmoid(pos_score-neg_score)\n",
    "    bpr_loss = -torch.mean(loss_term)\n",
    "    regularizer = 1./2*(user**2).sum() + 1./2*(pos_item**2).sum() + 1./2*(neg_item**2).sum()\n",
    "    regularizer = regularizer / batch_size\n",
    "    regularizer_loss = regularizer * regularization\n",
    "    return bpr_loss, regularizer_loss\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "train_generator = None\n",
    "test_dataset = None\n",
    "def test(model, train_module, test_dataloader, pool, eye):\n",
    "    model.eval()\n",
    "    correct = []\n",
    "    global train_generator\n",
    "    global test_dataset\n",
    "    train_generator = train_module\n",
    "\n",
    "    test_dataset = test_dataloader.dataset\n",
    "    with torch.no_grad():\n",
    "        user_embedding, item_embedding = model(train_module.L_c, eye)\n",
    "    n_user_batch = len(test_dataset.test_users) // train_module.batch_size + 1\n",
    "    total_hit_rate = []\n",
    "    for index, batch_test in enumerate(test_dataloader):\n",
    "        user_batch = batch_test.cuda()\n",
    "        user_embed = user_embedding[user_batch]\n",
    "        item_embed = item_embedding[torch.arange(0, train_module.n_item, dtype = torch.long).cuda()]\n",
    "        rate_batch = torch.matmul(user_embed, torch.transpose(item_embed, 0, 1))\n",
    "        # Train에 나왔던 item은 뽑히지 않게\n",
    "        make_pos_minus(user_batch, rate_batch, train_module.train_items)\n",
    "\n",
    "        topk_item = torch.topk(rate_batch, k=train_module.topk, dim=1).indices\n",
    "        topk_item = topk_item.detach().cpu().numpy()\n",
    "        \n",
    "        for i, user in enumerate(user_batch):\n",
    "            total_hit_rate += test_one_user(user.item(), topk_item[i])\n",
    "        # user_batch_rating = zip(topk_item, user_batch)\n",
    "        # result = pool.map(test_one_user, user_batch_rating)\n",
    "        length = float(len(total_hit_rate))\n",
    "    return np.sum(total_hit_rate)/length\n",
    "\n",
    "\n",
    "\n",
    "def make_pos_minus(user_batch, rate_batch, pos_train_item):\n",
    "    for i, user in enumerate(user_batch):\n",
    "        user = user.item()\n",
    "        pos_item = pos_train_item[user]\n",
    "        rate_batch[i, pos_item] = -10000.0 \n",
    "\n",
    "def test_one_user(user, topk_item):\n",
    "    # topk_item = x[0]\n",
    "    # user = x[1]\n",
    "    pos_test_items = test_dataset.test_items[user]\n",
    "    hit_list = []\n",
    "    for pos in pos_test_items:\n",
    "        if pos in topk_item:\n",
    "            hit_list.append(1)\n",
    "        else:\n",
    "            hit_list.append(0)\n",
    "    return hit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_embedding):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.n_embedding = n_embedding\n",
    "        self.user_embedding = nn.Embedding(self.n_user, self.n_embedding)\n",
    "        self.item_embedding = nn.Embedding(self.n_item, self.n_embedding)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        self.Front_Linear_List = nn.ModuleList()\n",
    "        self.Back_Linear_List = nn.ModuleList()\n",
    "        self.dropout_list = nn.ModuleList()\n",
    "        for _ in range(3):\n",
    "            self.Front_Linear_List.append(nn.Linear(64,64))\n",
    "            self.Back_Linear_List.append(nn.Linear(64,64))\n",
    "            self.dropout_list.append(nn.Dropout(p=0.5))\n",
    "\n",
    "    def forward(self, H, sparse_eye):\n",
    "        E_l_embedding = torch.cat((self.user_embedding.weight, self.item_embedding.weight), dim = 0)\n",
    "        all_embedding = [E_l_embedding]\n",
    "        H_I = H+sparse_eye\n",
    "        for i in range(3):\n",
    "            Front = torch.sparse.mm(H_I, E_l_embedding)\n",
    "            Front_cal = nn.functional.leaky_relu(self.Front_Linear_List[i](Front))\n",
    "            Back = torch.mul(E_l_embedding, Front)\n",
    "            Back = nn.functional.leaky_relu(self.Back_Linear_List[i](Back))\n",
    "            E_l_embedding = Front_cal + Back\n",
    "            E_l_embedding = self.dropout_list[i](E_l_embedding)\n",
    "            normalize_embed = nn.functional.normalize(E_l_embedding, p=2, dim=1)\n",
    "            all_embedding += [normalize_embed]\n",
    "        \n",
    "        all_embedding = torch.cat(all_embedding, dim=1)\n",
    "        user_embedding, item_embedding = torch.split(all_embedding, [self.n_user, self.n_item], dim=0)\n",
    "        return user_embedding, item_embedding\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpinionTest(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.n_test = 0\n",
    "        self.test_items = {}\n",
    "        self.path = config['path']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.test_items.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.test_users[index]\n",
    "\n",
    "    def load_test(self):\n",
    "        with open(self.path+'/test.txt') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) == 0:\n",
    "                    break\n",
    "                l = l.strip('\\n')\n",
    "                items = [int(i) for i in l.split(' ')]\n",
    "                self.n_test += len(items[1:])\n",
    "                self.test_items[items[0]] = items[1:]\n",
    "        self.test_users = torch.LongTensor(list(self.test_items.keys()))\n",
    "\n",
    "        \n",
    "\n",
    "class EpinionData(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.path = config['path']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.topk = config['topk']\n",
    "        self.n_user = 0\n",
    "        self.n_item = 0\n",
    "        self.n_train = 0\n",
    "        self.n_test = 0\n",
    "        self.train_items = {}\n",
    "        self.test_items = {}\n",
    "        self.R = sp.dok_matrix((10000000, 10000000), dtype=np.float32)\n",
    "        self.n_neg_item = config['neg_item']\n",
    "        self.pos_item = []\n",
    "        self.neg_item = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.exist_users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.exist_users[index], self.pos_item[index], self.neg_item[index]\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        # self.item_list = pd.read_table(self.path+'/item_list.txt', sep = '\\t', header=0)\n",
    "        # self.user_list = pd.read_table(self.path+'/user_list.txt', sep = '\\t', header=0)\n",
    "        self.exist_users = []\n",
    "        \n",
    "        with open(self.path+'/train.txt') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 2:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.R[uid, items] = 1.0\n",
    "                    self.train_items[uid] = items\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_item = max(self.n_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.n_train += len(items)\n",
    "        \n",
    "        self.n_user += 1\n",
    "        self.n_item += 1\n",
    "        self.R.resize((self.n_user, self.n_item))\n",
    "        \n",
    "      \n",
    "        self.Item_By_Item = sp.dok_matrix((self.n_item, self.n_item), dtype = np.float32)\n",
    "        self.User_By_User = sp.dok_matrix((self.n_user, self.n_user), dtype = np.float32)\n",
    "        self.H = sp.dok_matrix((self.n_user+self.n_item, self.n_user+self.n_item), dtype = np.float32)\n",
    "        self.normalize_H()\n",
    "               \n",
    "    def normalize_H(self):\n",
    "        self.H = self.H.tolil()\n",
    "        self.R = self.R.tolil()\n",
    "        self.H[:self.n_user, self.n_user:] = self.R\n",
    "        self.H[self.n_user:, :self.n_user] = self.R.T\n",
    "        # item과 user matrix를 넣으면 됨.\n",
    "        # self.H[:self.n_user, :self.n_user] = User Matrix\n",
    "        # self.H[self.n_user:, self.n_user:] = Item Matrix\n",
    "        self.H = self.H.todok()\n",
    "\n",
    "        # Normalize \n",
    "        rowsum = np.array(self.H.sum(1))\n",
    "        D_inv = np.power(rowsum, -1/2).flatten()\n",
    "        D_inv[np.isinf(D_inv)] = 0.0\n",
    "        D_ = sp.diags(D_inv)\n",
    "        self.L_c = (D_.dot(self.H)).dot(D_)\n",
    "        self.L_c = sparse_mx_to_torch_sparse_tensor(self.L_c).cuda()\n",
    "\n",
    "    def make_batch_sampling(self):\n",
    "        pos_item, neg_item = [], []\n",
    "        for user in self.exist_users:\n",
    "            pos_item += [random.choice(self.train_items[user])]\n",
    "            neg_item += self.neg_sampling(user)\n",
    "        self.pos_item = np.asarray(pos_item)\n",
    "        self.neg_item = np.asarray(neg_item).squeeze(1)\n",
    "\n",
    "\n",
    "    def neg_sampling(self, user):\n",
    "        neg = []\n",
    "        while True:\n",
    "            if len(neg) >= self.n_neg_item:\n",
    "                break\n",
    "            rand = np.random.randint(0, self.n_item, 1)\n",
    "            if rand not in self.train_items[user] and rand not in neg:\n",
    "                neg.append(rand)\n",
    "        return neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74064dccf20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user의 수 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"user의 수 : \",len(train_data.exist_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction = 0\n",
    "item_set = set()\n",
    "for k,v in data.train_items.items():\n",
    "    item_set.update(v)\n",
    "    interaction += len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "item의 수 :  37279\ninteraction의 수 :  99763\n"
     ]
    }
   ],
   "source": [
    "print(\"item의 수 : \",len(item_set))\n",
    "print(\"interaction의 수 : \", interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}